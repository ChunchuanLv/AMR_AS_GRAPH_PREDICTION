from utility.constants import *
from utility.amr import *
from utility.reader import *
from utility.Rules import *
from utility.pickle_helper import *
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from numpy import array
import json
import codecs
import sys
import os
import lasagne.init
from nltk.metrics.distance import edit_distance
from nltk.tag import StanfordPOSTagger

import threading

from concurrent.futures import *
max_workers=24

st_pos = []
pos_locks = []

embeddings_f = Pickle_Helper("data/embeddings") 
non_rule_set_f = Pickle_Helper("data/non_rule_set") 
softmax_short_list = Pickle_Helper("data/softmax_short_list") 
to_id_f= Pickle_Helper("data/to_id_f") 
rule_f= Pickle_Helper("data/rule_f") 


def unmixe(mixed,threshold = 5):
    high_frequency_concept = dict()
    low_frequency_concept = dict()
    text_num = dict()
    for i in mixed:
      #  print (i)
        if  i.is_constant() :
            text_num[i] = mixed[i]
        elif mixed[i][0] > threshold:
            high_frequency_concept[i] = mixed[i]
        else:
            low_frequency_concept[i] = mixed[i]
        
    return text_num,high_frequency_concept,low_frequency_concept

#or (s1 in s2 and len(s1) > 4)or (s2 in s1 and len(s2) > 4)
def checkMatch(s1,s2):
    if s2 in s1 or (s1 in s2 and len(s1)>3):  return True
    if s1.endswith("ily") and s1[:-3]+"ly"==s2:  return True
    if s1.endswith("ing") and s1[:-3]+"e"==s2:  return True
    if s1.endswith("ical") and s1[:-4]+"y"==s2:  return True
    if (1.0*edit_distance(s1,s2)/min(len(s1),len(s2)) < 0.25) : return True
    return False

def add_one_concept(snt_token,c_t,c):
    for tokens in snt_token:
        for t in tokens:
            t = t.lower()
            if checkMatch(t,c_t):
                if rl.add_lemmatize_cheat(t,c_t):
                    print (t,wordnet_lemmatizer.lemmatize(t),c_t,c)
    
def try_add_lemmatize_cheat(rl,non_rule_set):
    for c in non_rule_set.keys():
        if not c.is_constant() :
            if c.is_frame():
                c_t = re.sub(c.RE_FRAME_NUM,"",c.__str__())
            else:
                c_t = c.__str__()
            freq =  non_rule_set[c][0]
            snt_token = non_rule_set[c][1]
            if ("-" in c_t and freq <threshold and False):
                c_ts = c_t.split("-")
                broken_concepts[c] = []
                for t in c_ts:
                    broken_concepts[c].append(Concept(t))
                    add_one_concept(snt_token,t,c)
            else:
                add_one_concept(snt_token,c_t,c)
            
    

# Creating ReUsable Object
wordnet_lemmatizer = WordNetLemmatizer()
threshold = 10
rl = Rules()
broken_concepts = dict()
initializer = lasagne.init.Uniform()
non_rule_set_last = non_rule_set_f.load()["non_rule_set"]
try_add_lemmatize_cheat(rl,non_rule_set_last)
#436 78 148 210

#441 78 151 212

rule_f.dump(rl,"rule")

rule_f.save()



rl_saved_f = Pickle_Helper("data/rules") 

rl_saved_f.dump(rl,"rules")

rl_saved_f.save()

non_rule_set = dict()

lock = threading.Lock()
def add_count(store,new,additional=None):
    lock.acquire()
    for i in new:
        if not i in store:
            store[i] = [1,[additional]]
        else:
            store[i][0] = store[i][0] + 1 
            store[i][1].append(additional)
    lock.release()

def handle_sentence(snt_token,pos,amr_t,n):
    
    if n % 100 == 0:
        print (n)
    n = n % max_workers
    amr = AMR(amr_t)
    # print (snt)
    add_count(non_rule_set,rl.get_none_rule(snt_token,amr,pos),snt_token)
    
def readFile(filepath):
    executor = ThreadPoolExecutor(max_workers)
    
    with open(filepath.replace(".txt",".json"),'r') as data_file:    
        all_data = json.load(data_file)
    all_core_exceptions = []
    all_non_core_exceptions = []

    n = 0
    non_core_of_bad = 0
    core_bad = 0
    for data in all_data:
        n=n+1
        snt_token = data["snt_token"] 
        pos =data["pos"]
        amr_t = data["amr"]
     #   handle_sentence(snt_token,pos,amr_t,n)
        executor.submit( handle_sentence, snt_token,pos,amr_t,n )
    executor.shutdown()
    return n

def initial_embedding():
    concept_embedding[Concept(TOP)] = initializer((con_dim,))
    concept_embedding[Concept(END)] = initializer((con_dim,))
    lemma_embedding[UNK] = initializer((le_dim,))
    word_embedding[UNK] = lemma_embedding[UNK]
    for i, line in enumerate(open(embed_path, 'r')):
        parts = line.rstrip().split()
        word_embedding[parts[0]] = list(map(float, parts[1:]))
        le = wordnet_lemmatizer.lemmatize(parts[0],'v')
        if le not in lemma_embedding:
            lemma_embedding[le] = initializer((le_dim,))

def add_embedding(snt_token,lemmas,a):
    for i in range(len(snt_token)):
        if snt_token[i] not in word_embedding:
            word_embedding[snt_token[i]] = initializer((w_dim,))
        if lemmas[i] not in lemma_embedding:
            lemma_embedding[lemmas[i]] = initializer((le_dim,))
    for c in a.constants():
        if c not in concept_embedding:
            concept_embedding[c] = initializer((con_dim,))
    for c_pair in a.concepts():
        c = c_pair[1]
        if c not in concept_embedding:
            concept_embedding[c] = initializer((con_dim,))
                
def to_dict_and_embeds(embedding):
    embed_size = len(embedding)
    dictionary = dict()
    id_back = dict()
    embeds_l = []
    for k, v in embedding.items():
        dictionary[k] = len(dictionary)
        id_back[dictionary[k]] = k
        embeds_l.append(v)
    embeds = array( embeds_l)
    return dictionary,id_back,embedsFrames_Reader


def add_short_list_concept(lemmas,a):
    for c in a.constants():
        for le in lemmas:
            if c.is_str() and checkMatch(c.__str__().lower()[1:-1],le):
                add_concept(lemmas_to_concept,le,c)
            elif checkMatch(c.__str__().lower(),le):
                add_concept(lemmas_to_concept,le,c)
    RE_FRAME_NUM = re.compile(r'-\d\d$')
    for c in a.concepts():
        l = RE_FRAME_NUM.sub("",c[1].__str__())
        for le in lemmas:
            if  checkMatch(l,le):
                add_concept(lemmas_to_concept,le,c[1])
    return

def read_frame_roles():
    f_r = Coarse_Frames_Reader()
    return f_r.get_frames()
    
def read_resource_files(lemmas_to_concept):
    read_91(lemmas_to_concept,have_org_role,"have-org-role-91")
    read_91(lemmas_to_concept,have_rel_role,"have-rel-role-91")
    read_veb(lemmas_to_concept)
    add_concept(lemmas_to_concept,"no",AMRNumber("-"))
    add_concept(lemmas_to_concept,"not",AMRNumber("-"))
    add_concept(lemmas_to_concept,"non",AMRNumber("-"))
    add_concept(lemmas_to_concept,".",Concept(END))
    return lemmas_to_concept

#add self here and normalize
def turn_text_list_to_index_list(lemmas_to_concept,lemma_dict,concept_dict):
    index_to_index = dict()
    for le, cons in lemmas_to_concept.items():
        print (le,cons.keys())
        le_index = lemma_dict[le]
        print (le_index)
        cons_indexes = sorted([concept_dict[con] for con in cons.keys()])
        index_to_index[le_index] = array(cons_indexes)
    return index_to_index

def add_resource_embed(short_list,lemma_embedding,concept_embedding):
    for k in short_list.keys():
        if k not in lemma_embedding:
            lemma_embedding[k] = initializer((le_dim,))
        for c in short_list[k].keys():
            if c not in concept_embedding:
                concept_embedding[c] = initializer((con_dim,))
word_embedding = dict()
lemma_embedding = dict()
concept_embedding = dict()
short_list = dict()

#lemmas_to_concept =  read_resource_files( f_r.get_frames())
for filepath in trainingFilesPath:
    print(("reading "+filepath.split("/")[-1]+"......"))
    n = readFile(filepath)
    print(("done reading "+filepath.split("/")[-1]+", "+str(n)+" sentences processed"))
    
text_num,high_non_text,low_non_text=unmixe(non_rule_set,threshold)
print (threshold,len(non_rule_set),len(text_num),len(high_non_text),len(low_non_text))
#print (low_non_text)
print (threshold,len(non_rule_set),len(text_num),len(high_non_text),len(low_non_text))
#print (len(concept_embedding))
#10 13276 6241 1806 5229
  
#read_mor(lemmas_to_concept) #essentially copying

non_rule_set_f.dump(non_rule_set,"non_rule_set")
embeddings_f.save()
to_id_f.save()
non_rule_set_f.save()
softmax_short_list.save()

